#!/usr/bin/env python3

import argparse
import json
import logging
import getpass
import requests
import traceback
import urllib3
import os
import sys

from queue import Queue
from threading import Thread

from sseclient import SSEClient

from rucio.client import Client
from rucio.rse import rsemanager as rsemgr

_DEFAULT_LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'

_LOGGER = logging.getLogger(__name__)

def _configure_logging():
    _LOGGER.setLevel(logging.DEBUG)

    ch = logging.StreamHandler()

    formatter = logging.Formatter(_DEFAULT_LOG_FORMAT)
    ch.setFormatter(formatter)

    _LOGGER.addHandler(ch)


def get_parser():
    '''
    Method to return the argparse parser.
    '''
    parser = argparse.ArgumentParser(description='Sample dCache SSE consumer')
    parser.add_argument('--endpoint',
                        default="https://discordia.desy.de:3880/api/v1/events",
                        help="The events endpoint.  This should be a URL like 'https://frontend.example.org:3880/api/v1/events'.")
    # parser.add_argument('--user', metavar="NAME", default=getpass.getuser(),
    #                    help="The dCache username.  Defaults to the current user's name.")
    # parser.add_argument('--password', default=None,
    #                    help="The dCache password.  Defaults to prompting the user.")
    parser.add_argument('--inotify', default=None, metavar="PATH",
                        help="Subscribe to events on PATH.")
    parser.add_argument(
        '--proxy', dest='proxy',
        default=os.environ.get('X509_USER_PROXY', '/tmp/x509up_u' + str(os.getuid())),
        help='Client X509 proxy file.')
    parser.add_argument('--source', action='store', help='The source RSE where the replicas should be registered.', required=True)
    parser.add_argument('--scope', dest='scope', action='store', help='Scope name.', required=True)
    parser.add_argument('--destination', action='store', default=None, help='The destination RSE where the replicas should be replicated.')
    return parser

def do_stuff(q, storage, source, destination, scope, proxy):
    rucio_client = Client()
    s = requests.Session()
    # s.auth = (user,pw)
    s.cert = proxy
    s.verify = False
    urllib3.disable_warnings()
    while True:
        try:
            name, new_file = q.get()
            response = s.get(new_file, headers={'Want-Digest': 'adler32'})
            adler32 = response.headers['Digest'].replace('adler32=', '')
            bytes = response.headers['Content-Length']
            replica  = {
                'scope': scope,
                'name': name,
                'pfn': new_file,
                'bytes': int(bytes),
                'adler32': adler32}

            rucio_client.add_replicas(
                rse=source,
                files=[replica])

            if destination:
                rucio_client.add_replication_rule(
                    dids=[{'scope': scope, 'name': name}],
                    account='root',
                    copies=2,
                    rse_expression='%s|%s'%(source, destination),
                    grouping='NONE',
                    weight=None,
                    lifetime=None,
                    locked=False)
                _LOGGER.info('Added replica and rules for file' + scope+ ':' + name)
            else:
                rucio_client.add_replication_rule(
                    dids=[{'scope': scope, 'name': name}],
                    account='root',
                    copies=1,
                    rse_expression=source,
                    grouping='NONE',
                    weight=None,
                    lifetime=None,
                    locked=False)
                _LOGGER.info('Added replica and rule for file' + scope+ ':' + name)
        except:
            _LOGGER.error(traceback.format_exc())

        finally:
            q.task_done()


def _main():
    '''
    main function
    '''
    parser = get_parser()
    args = parser.parse_args()
    # user = vars(args).get("user")
    # pw = vars(args).get("password")
    proxy = vars(args).get("proxy")
    scope = vars(args).get("scope")
    source = vars(args).get("source")
    destination = vars(args).get("destination")
    # get storage path
    rse_settings = rsemgr.get_rse_info(source)
    protocols = rsemgr.get_protocols_ordered(rse_settings=rse_settings, operation='write')
    storage='%(scheme)s://%(hostname)s:%(port)s' % protocols[0]
    _LOGGER.info('Storage url: ' + storage)

    # if not pw:
    #    pw = getpass.getpass("Please enter dCache password for user " + user + ": ")

    new_files = Queue(maxsize=0)
    worker = Thread(target=do_stuff, args=(new_files, storage, source, destination, scope, proxy))
    worker.setDaemon(True)
    worker.start()

    s = requests.Session()
    # s.auth = (user,pw)
    s.cert = proxy
    ## FIXME -- here we (in effect) disable all security from TLS.
    ##          This should be fixed by allowing the user to specify
    ##          an alternative trust-store
    s.verify = False
    urllib3.disable_warnings()

    response = s.post(vars(args).get("endpoint") + '/channels')
    if response.status_code != 201:
        print(response)
        print(response.headers)
        sys.exit(-1)

    channel = response.headers['Location']
    _LOGGER.info("Channel is {}".format(channel))

    try:
        path = vars(args).get("inotify")
        if path:
            r = s.post(format(channel) + "/subscriptions/inotify", json={"path" : path})
            if r.status_code != 201:
                print(r)
                print(r.headers)
                sys.exit(-1)
            watch = r.headers['Location']
            _LOGGER.info("Watch on {} is {}".format(path, watch))

            messages = SSEClient(channel, session=s)
            for msg in messages:
                # print "Event {}:".format(msg.id)
                # print "    event: {}".format(msg.event)
                # print "    data: {}".format(msg.data)
                data = json.loads(msg.data)
                if data['event']['mask'] == ['IN_ATTRIB']:
                    # new file
                    name = data['event']['name']
                    new_file = storage + path + '/' + name
                    _LOGGER.info('New file detected: ' + new_file)
                    new_files.put((name, new_file))

    except KeyboardInterrupt:
        print("Interrupting...")

    finally:
        print("Deleting channel")
        s.delete(channel)

if __name__ == '__main__':
    _configure_logging()
    _main()
